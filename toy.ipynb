{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sihun.cha/miniconda3/envs/hallo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat\n",
    "from typing import Callable, List, Optional, Union\n",
    "import torchvision as tv\n",
    "\n",
    "from diffusers import AutoencoderKL, DDIMScheduler\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "\n",
    "from exp.animate.face_animate import FaceAnimatePipeline\n",
    "from exp.animate.face_animate_static import StaticPipeline\n",
    "\n",
    "from exp.models.audio_proj import AudioProjModel\n",
    "from exp.models.face_locator import FaceLocator\n",
    "from exp.models.mutual_self_attention import ReferenceAttentionControl\n",
    "from exp.models.unet_2d_condition import UNet2DConditionModel\n",
    "from exp.models.unet_3d import UNet3DConditionModel\n",
    "from exp.models.image_proj import ImageProjModel\n",
    "from emo.models.speed_encoder import SpeedEncoder\n",
    "\n",
    "from exp.datasets.audio_processor import AudioProcessor\n",
    "from exp.datasets.image_processor import ImageProcessor\n",
    "# from exp.datasets.mask_image import EMODataset\n",
    "# from exp.datasets.talk_video import TalkingVideoDataset\n",
    "\n",
    "from exp.utils.util import tensor_to_video\n",
    "from exp.utils.params import (\n",
    "    param_optim, \n",
    "    create_optim_params, \n",
    "    negate_params, \n",
    "    create_optimizer_params, \n",
    "    handle_trainable_modules,\n",
    "    freeze_params\n",
    ")\n",
    "from exp.models.lora_handler import LoraHandler\n",
    "\n",
    "# from scripts.train_stage3_emo import Net, load_config, process_audio_emb\n",
    "from scripts.train_stage_exp_lora import Net, load_config, process_audio_emb\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_dir = '/home/sihun.cha/Work/hallo/.cache/output_7.mp4'\n",
    "# Video('/home/sihun.cha/Work/hallo/.cache/output_7.mp4')\n",
    "from IPython.display import HTML\n",
    "\n",
    "# HTML(\"\"\"\n",
    "#     <video alt=\"test\" controls>\n",
    "#         <source src=\"/home/sihun.cha/Work/hallo/.cache/output_7.mp4\">\n",
    "#     </video>\n",
    "# \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"./configs/train/exp/stage2.yaml\")\n",
    "if cfg.weight_dtype == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "elif cfg.weight_dtype == \"bf16\":\n",
    "    weight_dtype = torch.bfloat16\n",
    "elif cfg.weight_dtype == \"fp32\":\n",
    "    weight_dtype = torch.float32\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Do not support weight dtype: {cfg.weight_dtype} during training\"\n",
    "    )\n",
    "\n",
    "sched_kwargs = OmegaConf.to_container(cfg.noise_scheduler_kwargs)\n",
    "if cfg.enable_zero_snr:\n",
    "    sched_kwargs.update(\n",
    "        rescale_betas_zero_snr=True,\n",
    "        timestep_spacing=\"trailing\",\n",
    "        prediction_type=\"v_prediction\",\n",
    "    )\n",
    "val_noise_scheduler = DDIMScheduler(**sched_kwargs)\n",
    "sched_kwargs.update({\"beta_schedule\": \"scaled_linear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'center_input_sample': False} were passed to UNet3DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load motion module params from pretrained_models/motion_module/mm_sd_v15_v2.ckpt\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'center_input_sample': False, 'out_channels': 4} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint were not used when initializing UNet2DConditionModel: \n",
      " ['conv_norm_out.bias, conv_norm_out.weight, conv_out.bias, conv_out.weight']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# use_pipeline = 'emo'\n",
    "use_pipeline = 'hallo'\n",
    "\n",
    "denoising_unet = UNet3DConditionModel.from_pretrained_2d(\n",
    "    cfg.base_model_path,\n",
    "    cfg.motion_module_path,\n",
    "    subfolder=\"unet\",\n",
    "    unet_additional_kwargs=OmegaConf.to_container(\n",
    "        cfg.unet_additional_kwargs),\n",
    "    use_landmark=False,\n",
    "    vis_atttn=True,\n",
    ").to(device=\"cuda\", dtype=weight_dtype)\n",
    "print(\"3\")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    cfg.vae_model_path\n",
    ").to(\"cuda\", dtype=weight_dtype)\n",
    "print(\"1\")\n",
    "reference_unet = UNet2DConditionModel.from_pretrained(\n",
    "    cfg.base_model_path,\n",
    "    subfolder=\"unet\",\n",
    ").to(device=\"cuda\", dtype=weight_dtype)\n",
    "print(\"2\")\n",
    "\n",
    "if use_pipeline == 'hallo':\n",
    "    face_locator = FaceLocator(\n",
    "        conditioning_embedding_channels=320,\n",
    "        conditioning_channels=3,\n",
    "    ).to(device=\"cuda\", dtype=weight_dtype)\n",
    "    print(\"5\")\n",
    "    audioproj = AudioProjModel(\n",
    "        seq_len=5,\n",
    "        blocks=12,\n",
    "        channels=768,\n",
    "        intermediate_dim=512,\n",
    "        output_dim=768,\n",
    "        context_tokens=32,\n",
    "    ).to(device=\"cuda\", dtype=weight_dtype)\n",
    "    print(\"6\")\n",
    "\n",
    "    imageproj = ImageProjModel(\n",
    "        cross_attention_dim=denoising_unet.config.cross_attention_dim,\n",
    "        clip_embeddings_dim=512,\n",
    "        clip_extra_context_tokens=4,\n",
    "    ).to(device=\"cuda\", dtype=weight_dtype)\n",
    "\n",
    "##### for EMO (but no checkpoints...) #########################\n",
    "if use_pipeline == 'emo':\n",
    "    image_enc = CLIPVisionModelWithProjection.from_pretrained(\n",
    "        cfg.image_encoder_path,\n",
    "    ).to(device=\"cuda\", dtype=weight_dtype)\n",
    "    print(\"4\")\n",
    "    speed_enc = SpeedEncoder(\n",
    "        num_speed_buckets=8, # ? not sure if it is correct\n",
    "        speed_embedding_dim=768,\n",
    "    ).to(device=\"cuda\", dtype=weight_dtype)\n",
    "    print(\"7\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### No gradients !!!!\n",
    "# vae.requires_grad_(False)\n",
    "# image_enc.requires_grad_(False)\n",
    "# reference_unet.requires_grad_(False)\n",
    "# denoising_unet.requires_grad_(False)\n",
    "# face_locator.requires_grad_(False)\n",
    "# audioproj.requires_grad_(False)\n",
    "# speed_enc.requires_grad_(False)\n",
    "freeze_params([\n",
    "    vae, \n",
    "    reference_unet, \n",
    "    denoising_unet,\n",
    "    face_locator,\n",
    "    imageproj,\n",
    "    audioproj\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for param in [*denoising_unet.parameters()]:\n",
    "    if param.requires_grad:\n",
    "        i+=1\n",
    "        #print(param)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_control_writer = ReferenceAttentionControl(\n",
    "    reference_unet,\n",
    "    do_classifier_free_guidance=False,\n",
    "    mode=\"write\",\n",
    "    fusion_blocks=\"full\",\n",
    ")\n",
    "reference_control_reader = ReferenceAttentionControl(\n",
    "    denoising_unet,\n",
    "    do_classifier_free_guidance=False,\n",
    "    mode=\"read\",\n",
    "    fusion_blocks=\"full\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Net(\n",
    "#         reference_unet,\n",
    "#         denoising_unet,\n",
    "# ).to(dtype=weight_dtype)\n",
    "\n",
    "net = Net(\n",
    "    reference_unet,\n",
    "    denoising_unet,\n",
    "    face_locator,\n",
    "    # reference_control_writer,\n",
    "    # reference_control_reader,\n",
    "    None,\n",
    "    None,\n",
    "    imageproj,\n",
    "    audioproj,\n",
    ").to(dtype=weight_dtype)\n",
    "\n",
    "m,u = net.load_state_dict(\n",
    "    torch.load(\n",
    "        os.path.join(cfg.audio_ckpt_dir, \"net.pth\"),\n",
    "        map_location=\"cpu\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # attn feature map analysis\n",
    "# from exp.utils.attention_map import (\n",
    "#     register_cross_attention_hook,\n",
    "# )\n",
    "# denoising_unet, attn_maps = register_cross_attention_hook(denoising_unet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select layers to visualize attn feature map\n",
    "# for name, module in denoising_unet.named_modules():\n",
    "#     # if not name.split('.')[-1].startswith('attn2'):\n",
    "    \n",
    "#     if name.split('.')[-1] == 'attn2':\n",
    "#         print(name)\n",
    "#     if name.split('.')[-1] == 'attention_blocks':\n",
    "#         print(name)\n",
    "#         # print(module.processor)\n",
    "        \n",
    "#     # if name.find('attn2') > 0:\n",
    "#     #     print(name)\n",
    "#     # else:\n",
    "#     #     continue\n",
    "\n",
    "# # if isinstance(net.denoising_unet, UNet3DConditionModel):\n",
    "# #     print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down_blocks.0.attentions.0.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "down_blocks.0.attentions.1.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "down_blocks.1.attentions.0.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "down_blocks.1.attentions.1.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "down_blocks.2.attentions.0.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "down_blocks.2.attentions.1.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "up_blocks.1.attentions.0.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "up_blocks.1.attentions.1.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "up_blocks.1.attentions.2.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "up_blocks.2.attentions.0.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "up_blocks.2.attentions.1.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "up_blocks.2.attentions.2.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "up_blocks.3.attentions.0.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "up_blocks.3.attentions.1.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "up_blocks.3.attentions.2.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "mid_block.attentions.0.transformer_blocks.0 TemporalBasicTransformerBlock(\n",
      "  (attn1): Attention(\n",
      "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn2): Attention(\n",
      "    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "    (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
      "    (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
      "    (to_out): ModuleList(\n",
      "      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): FeedForward(\n",
      "    (net): ModuleList(\n",
      "      (0): GEGLU(\n",
      "        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "      )\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "----------\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# denoising_unet\n",
    "from exp.models.lora import LoraInjectedLinear, LoraInjectedConv2d, LoraInjectedConv3d\n",
    "search_class=[nn.Linear, nn.Conv2d, nn.Conv3d]\n",
    "#search_class=[LoraInjectedLinear, LoraInjectedConv2d, LoraInjectedConv3d]\n",
    "# ancestor_class = ['VersatileAttention'] # motion_module attn\n",
    "# ancestor_class = ['TemporalTransformerBlock'] # motion_module\n",
    "ancestor_class = ['TemporalBasicTransformerBlock'] # spatial\n",
    "\n",
    "if ancestor_class is not None:\n",
    "    # ancestors = (\n",
    "    #     module\n",
    "    #     for name, module in denoising_unet.named_modules()\n",
    "    #     if module.__class__.__name__ in ancestor_class # and ('transformer_in' not in name)\n",
    "    # )\n",
    "    ancestors = []\n",
    "    for name, module in denoising_unet.named_modules():\n",
    "        if module.__class__.__name__ in ancestor_class:\n",
    "            ancestors.append(module)\n",
    "            # print(name, module.__class__.__name__)\n",
    "            print(name, module)\n",
    "            print('-'*10)\n",
    "    print('1')\n",
    "else:\n",
    "    # this, incase you want to naively iterate over all modules.\n",
    "    ancestors = [module for module in denoising_unet.modules()]\n",
    "    print('2')\n",
    "    \n",
    "# for name, module in denoising_unet.named_modules():\n",
    "#     # print(name)\n",
    "#     # print('-----')\n",
    "#     if any([isinstance(module, _class) for _class in search_class]):\n",
    "#         print(module)\n",
    "#     # for nn, mm in denoising_unet.named_modules():\n",
    "#     #     print(nn)\n",
    "#     #     print('-----')\n",
    "#     #     print(mm)\n",
    "\n",
    "#     # # if module.__class__.__name__ == 'motion_modules':\n",
    "#     # break\n",
    "# # module #.__dict__.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=320, out_features=2560, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=320, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=2560, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=1280, out_features=320, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=320, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=320, out_features=2560, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=320, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=2560, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=1280, out_features=320, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=320, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=640, out_features=5120, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=640, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=5120, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=2560, out_features=640, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=2560, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=640, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=640, out_features=5120, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=640, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=5120, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=2560, out_features=640, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=2560, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=640, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=1280, out_features=10240, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=10240, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=5120, out_features=1280, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=5120, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=1280, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=1280, out_features=10240, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=10240, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=5120, out_features=1280, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=5120, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=1280, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=1280, out_features=10240, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=10240, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=5120, out_features=1280, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=5120, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=1280, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=1280, out_features=10240, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=10240, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=5120, out_features=1280, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=5120, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=1280, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=1280, out_features=10240, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=10240, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=5120, out_features=1280, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=5120, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=1280, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=1280, out_features=10240, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=10240, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=5120, out_features=1280, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=5120, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=1280, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=1280, out_features=10240, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=10240, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=5120, out_features=1280, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=5120, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=1280, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=1280, out_features=10240, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=10240, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=5120, out_features=1280, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=5120, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=1280, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=1280, out_features=10240, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=10240, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=5120, out_features=1280, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=5120, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=1280, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=1280, out_features=10240, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=10240, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=5120, out_features=1280, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=5120, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=1280, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=640, out_features=5120, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=640, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=5120, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=2560, out_features=640, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=2560, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=640, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=640, out_features=5120, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=640, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=5120, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=2560, out_features=640, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=2560, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=640, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=640, out_features=5120, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=640, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=5120, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=2560, out_features=640, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=2560, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=640, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=320, out_features=2560, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=320, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=2560, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=1280, out_features=320, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=320, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=320, out_features=2560, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=320, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=2560, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=1280, out_features=320, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=320, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=320, out_features=2560, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=320, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=2560, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=1280, out_features=320, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=320, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n",
      "Linear(in_features=1280, out_features=10240, bias=True) ff.net.0.proj.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] linear\n",
      "Linear(in_features=1280, out_features=32, bias=False) ff.net.0.proj.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_down\n",
      "Linear(in_features=32, out_features=10240, bias=False) ff.net.0.proj.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '0', 'proj'] lora_up\n",
      "Linear(in_features=5120, out_features=1280, bias=True) ff.net.2.linear <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] linear\n",
      "Linear(in_features=5120, out_features=32, bias=False) ff.net.2.lora_down <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_down\n",
      "Linear(in_features=32, out_features=1280, bias=False) ff.net.2.lora_up <class 'torch.nn.modules.linear.Linear'>\n",
      "['ff', 'net', '2'] lora_up\n"
     ]
    }
   ],
   "source": [
    "for ancestor in ancestors:\n",
    "    for name, module in ancestor.named_modules():\n",
    "        # print(name)\n",
    "        # print('-----')\n",
    "        # if any([isinstance(module, _class) for _class in search_class]):\n",
    "        #     print(module, name)\n",
    "        for _class in search_class:\n",
    "            if any([isinstance(module, _class)]):\n",
    "                if ('attn1' in name or 'ff' in name):\n",
    "                    print(module, name, _class)\n",
    "                    *path, n_ = name.split(\".\")\n",
    "                    print(path, n_)\n",
    "        # for nn, mm in denoising_unet.named_modules():\n",
    "        #     print(nn)\n",
    "        #     print('-----')\n",
    "        #     print(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ancestor in ancestors:\n",
    "    for name, module in ancestor.named_modules():\n",
    "        # print(name)\n",
    "        # print('-----')\n",
    "        # if any([isinstance(module, _class) for _class in search_class]):\n",
    "        #     print(module, name)\n",
    "        for _class in search_class:\n",
    "            if any([isinstance(module, _class)]):\n",
    "                if ('attn1' in name or 'attn2' in name or 'ff' in name):\n",
    "                    print(module, _class)\n",
    "                    *path, n_ = name.split(\".\")\n",
    "                    print(path, n_)\n",
    "        # for nn, mm in denoising_unet.named_modules():\n",
    "        #     print(nn)\n",
    "        #     print('-----')\n",
    "        #     print(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ancestor in ancestors:\n",
    "    for fullname, module in ancestor.named_modules():\n",
    "        if any([isinstance(module, _class) for _class in search_class]):\n",
    "            print(fullname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoising_unet.modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using lora\n",
      "inject\n",
      "attn1.to_q\n",
      "to_q\n",
      "attn1.to_k\n",
      "to_k\n",
      "attn1.to_v\n",
      "to_v\n",
      "attn1.to_out.0\n",
      "0\n",
      "attn2.to_q\n",
      "to_q\n",
      "attn2.to_k\n",
      "to_k\n",
      "attn2.to_v\n",
      "to_v\n",
      "attn2.to_out.0\n",
      "0\n",
      "ff.net.0.proj\n",
      "proj\n",
      "ff.net.2\n",
      "2\n",
      "attn1.to_q\n",
      "to_q\n",
      "attn1.to_k\n",
      "to_k\n",
      "attn1.to_v\n",
      "to_v\n",
      "attn1.to_out.0\n",
      "0\n",
      "attn2.to_q\n",
      "to_q\n",
      "attn2.to_k\n",
      "to_k\n",
      "attn2.to_v\n",
      "to_v\n",
      "attn2.to_out.0\n",
      "0\n",
      "ff.net.0.proj\n",
      "proj\n",
      "ff.net.2\n",
      "2\n",
      "attn1.to_q\n",
      "to_q\n",
      "attn1.to_k\n",
      "to_k\n",
      "attn1.to_v\n",
      "to_v\n",
      "attn1.to_out.0\n",
      "0\n",
      "attn2.to_q\n",
      "to_q\n",
      "attn2.to_k\n",
      "to_k\n",
      "attn2.to_v\n",
      "to_v\n",
      "attn2.to_out.0\n",
      "0\n",
      "ff.net.0.proj\n",
      "proj\n",
      "ff.net.2\n",
      "2\n",
      "attn1.to_q\n",
      "to_q\n",
      "attn1.to_k\n",
      "to_k\n",
      "attn1.to_v\n",
      "to_v\n",
      "attn1.to_out.0\n",
      "0\n",
      "attn2.to_q\n",
      "to_q\n",
      "attn2.to_k\n",
      "to_k\n",
      "attn2.to_v\n",
      "to_v\n",
      "attn2.to_out.0\n",
      "0\n",
      "ff.net.0.proj\n",
      "proj\n",
      "ff.net.2\n",
      "2\n",
      "attn1.to_q\n",
      "to_q\n",
      "attn1.to_k\n",
      "to_k\n",
      "attn1.to_v\n",
      "to_v\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m     lora_manager_temporal \u001b[38;5;241m=\u001b[39m LoraHandler(\n\u001b[1;32m      9\u001b[0m         use_unet_lora\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mlora\u001b[38;5;241m.\u001b[39muse_unet_lora, \n\u001b[1;32m     10\u001b[0m         unet_replace_modules\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m         ]\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     15\u001b[0m     lora_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./_tmp\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 17\u001b[0m     unet_lora_params_temporal, unet_negation_temporal \u001b[38;5;241m=\u001b[39m \u001b[43mlora_manager_temporal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_lora_to_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_unet_lora\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdenoising_unet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_manager_temporal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet_replace_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_unet_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/lora/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_rank\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot using lora\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hallo/lib/python3.10/site-packages/exp/models/lora_handler.py:235\u001b[0m, in \u001b[0;36mLoraHandler.add_lora_to_model\u001b[0;34m(self, use_lora, model, replace_modules, dropout, lora_path, r, scale)\u001b[0m\n\u001b[1;32m    223\u001b[0m lora_loader_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_lora_func_args(\n\u001b[1;32m    224\u001b[0m     lora_path,\n\u001b[1;32m    225\u001b[0m     use_lora,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m     scale\n\u001b[1;32m    232\u001b[0m )\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_lora:\n\u001b[0;32m--> 235\u001b[0m     params, negation, is_injection_hybrid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_lora_injection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplace_modules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_loader_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_loader_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_injection_hybrid:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lora(model, lora_path\u001b[38;5;241m=\u001b[39mlora_path, lora_loader_args\u001b[38;5;241m=\u001b[39mlora_loader_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/hallo/lib/python3.10/site-packages/exp/models/lora_handler.py:195\u001b[0m, in \u001b[0;36mLoraHandler.do_lora_injection\u001b[0;34m(self, model, replace_modules, bias, dropout, r, lora_loader_args)\u001b[0m\n\u001b[1;32m    192\u001b[0m is_injection_hybrid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    193\u001b[0m injector_args \u001b[38;5;241m=\u001b[39m lora_loader_args\n\u001b[0;32m--> 195\u001b[0m params, negation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_injector\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minjector_args\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# inject_trainable_lora_extended\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextract\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _up, _down \u001b[38;5;129;01min\u001b[39;00m extract_lora_ups_down(\n\u001b[1;32m    198\u001b[0m     model, \n\u001b[1;32m    199\u001b[0m     target_replace_module\u001b[38;5;241m=\u001b[39mREPLACE_MODULES):\n",
      "File \u001b[0;32m~/miniconda3/envs/hallo/lib/python3.10/site-packages/exp/models/lora.py:555\u001b[0m, in \u001b[0;36minject_trainable_lora_extended\u001b[0;34m(model, target_replace_module, r, loras, dropout_p, scale)\u001b[0m\n\u001b[1;32m    552\u001b[0m require_grad_params\u001b[38;5;241m.\u001b[39mappend(_module\u001b[38;5;241m.\u001b[39m_modules[name]\u001b[38;5;241m.\u001b[39mlora_down\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loras \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     _module\u001b[38;5;241m.\u001b[39m_modules[name]\u001b[38;5;241m.\u001b[39mlora_up\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(\u001b[43mloras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    556\u001b[0m     _module\u001b[38;5;241m.\u001b[39m_modules[name]\u001b[38;5;241m.\u001b[39mlora_down\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(loras\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# _module._modules[name].lora_up.weight.requires_grad = True\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# _module._modules[name].lora_down.weight.requires_grad = True\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "use_lora = True\n",
    "\n",
    "if use_lora:\n",
    "    print('using lora')\n",
    "    # denoising_unet.add_adapter(unet_lora_config)\n",
    "    # denoising_unet.unload_lora()\n",
    "\n",
    "    lora_manager_temporal = LoraHandler(\n",
    "        use_unet_lora=cfg.lora.use_unet_lora, \n",
    "        unet_replace_modules=[\n",
    "            \"TemporalBasicTransformerBlock\", # for spatial-attention\n",
    "            # \"TemporalTransformerBlock\", # for motion_module\n",
    "        ]\n",
    "    )\n",
    "    lora_path = './_tmp'\n",
    "\n",
    "    unet_lora_params_temporal, unet_negation_temporal = lora_manager_temporal.add_lora_to_model(\n",
    "        cfg.lora.use_unet_lora, \n",
    "        denoising_unet, \n",
    "        lora_manager_temporal.unet_replace_modules,\n",
    "        cfg.lora.lora_unet_dropout,\n",
    "        lora_path + '/lora/', \n",
    "        r=cfg.lora.lora_rank\n",
    "    )\n",
    "else:\n",
    "    print('not using lora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denoising_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for param in [*denoising_unet.parameters()]:\n",
    "    if param.requires_grad:\n",
    "        # print(True)\n",
    "        i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_lora=False\n",
    "# denoising_unet.get_submodule('down_blocks.0.motion_modules')\n",
    "if save_lora:\n",
    "    print('saving lora')\n",
    "    lora_manager_temporal.save_lora_weights(\n",
    "                        model=copy.deepcopy(net),\n",
    "                        save_path='./_tmp',\n",
    "                        step=0\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet3DConditionModel(\n",
       "  (conv_in): InflatedConv3d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (time_proj): Timesteps()\n",
       "  (time_embedding): TimestepEmbedding(\n",
       "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): CrossAttnDownBlock3D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer3DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): TemporalBasicTransformerBlock(\n",
       "              (attn1): Attention(\n",
       "                (to_q): LoraInjectedLinear(\n",
       "                  (linear): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (lora_down): Linear(in_features=320, out_features=32, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lora_up): Linear(in_features=32, out_features=320, bias=False)\n",
       "                  (selector): Identity()\n",
       "                )\n",
       "                (to_k): LoraInjectedLinear(\n",
       "                  (linear): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (lora_down): Linear(in_features=320, out_features=32, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lora_up): Linear(in_features=32, out_features=320, bias=False)\n",
       "                  (selector): Identity()\n",
       "                )\n",
       "                (to_v): LoraInjectedLinear(\n",
       "                  (linear): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (lora_down): Linear(in_features=320, out_features=32, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lora_up): Linear(in_features=32, out_features=320, bias=False)\n",
       "                  (selector): Identity()\n",
       "                )\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoraInjectedLinear(\n",
       "                    (linear): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (lora_down): Linear(in_features=320, out_features=32, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_up): Linear(in_features=32, out_features=320, bias=False)\n",
       "                    (selector): Identity()\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): LoraInjectedLinear(\n",
       "                  (linear): Linear(in_features=320, out_features=320, bias=False)\n",
       "                  (lora_down): Linear(in_features=320, out_features=32, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lora_up): Linear(in_features=32, out_features=320, bias=False)\n",
       "                  (selector): Identity()\n",
       "                )\n",
       "                (to_k): LoraInjectedLinear(\n",
       "                  (linear): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (lora_down): Linear(in_features=768, out_features=32, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lora_up): Linear(in_features=32, out_features=320, bias=False)\n",
       "                  (selector): Identity()\n",
       "                )\n",
       "                (to_v): LoraInjectedLinear(\n",
       "                  (linear): Linear(in_features=768, out_features=320, bias=False)\n",
       "                  (lora_down): Linear(in_features=768, out_features=32, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lora_up): Linear(in_features=32, out_features=320, bias=False)\n",
       "                  (selector): Identity()\n",
       "                )\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoraInjectedLinear(\n",
       "                    (linear): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (lora_down): Linear(in_features=320, out_features=32, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_up): Linear(in_features=32, out_features=320, bias=False)\n",
       "                    (selector): Identity()\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): LoraInjectedLinear(\n",
       "                      (linear): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                      (lora_down): Linear(in_features=320, out_features=32, bias=False)\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      (lora_up): Linear(in_features=32, out_features=2560, bias=False)\n",
       "                      (selector): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): LoraInjectedLinear(\n",
       "                    (linear): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                    (lora_down): Linear(in_features=1280, out_features=32, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_up): Linear(in_features=32, out_features=320, bias=False)\n",
       "                    (selector): Identity()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock3D(\n",
       "          (norm1): InflatedGroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): InflatedConv3d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): InflatedGroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): InflatedConv3d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (audio_modules): ModuleList(\n",
       "        (0-1): 2 x Transformer3DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): AudioTemporalBasicTransformerBlock(\n",
       "              (zero_conv_full): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_face): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_lip): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2_0): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (motion_modules): ModuleList(\n",
       "        (0-1): 2 x VanillaTemporalModule(\n",
       "          (temporal_transformer): TemporalTransformer3DModel(\n",
       "            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): TemporalTransformerBlock(\n",
       "                (attention_blocks): ModuleList(\n",
       "                  (0-1): 2 x VersatileAttention(\n",
       "                    (Module Info) Attention_Mode: Temporal, Is_Cross_Attention: False\n",
       "                    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (pos_encoder): PositionalEncoding(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (norms): ModuleList(\n",
       "                  (0-1): 2 x LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (ff_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample3D(\n",
       "          (conv): InflatedConv3d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnDownBlock3D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer3DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): TemporalBasicTransformerBlock(\n",
       "              (attn1): Attention(\n",
       "                (to_q): LoraInjectedLinear(\n",
       "                  (linear): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (lora_down): Linear(in_features=640, out_features=32, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lora_up): Linear(in_features=32, out_features=640, bias=False)\n",
       "                  (selector): Identity()\n",
       "                )\n",
       "                (to_k): LoraInjectedLinear(\n",
       "                  (linear): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (lora_down): Linear(in_features=640, out_features=32, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lora_up): Linear(in_features=32, out_features=640, bias=False)\n",
       "                  (selector): Identity()\n",
       "                )\n",
       "                (to_v): LoraInjectedLinear(\n",
       "                  (linear): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (lora_down): Linear(in_features=640, out_features=32, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lora_up): Linear(in_features=32, out_features=640, bias=False)\n",
       "                  (selector): Identity()\n",
       "                )\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoraInjectedLinear(\n",
       "                    (linear): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (lora_down): Linear(in_features=640, out_features=32, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_up): Linear(in_features=32, out_features=640, bias=False)\n",
       "                    (selector): Identity()\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): LoraInjectedLinear(\n",
       "                  (linear): Linear(in_features=640, out_features=640, bias=False)\n",
       "                  (lora_down): Linear(in_features=640, out_features=32, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lora_up): Linear(in_features=32, out_features=640, bias=False)\n",
       "                  (selector): Identity()\n",
       "                )\n",
       "                (to_k): LoraInjectedLinear(\n",
       "                  (linear): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (lora_down): Linear(in_features=768, out_features=32, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lora_up): Linear(in_features=32, out_features=640, bias=False)\n",
       "                  (selector): Identity()\n",
       "                )\n",
       "                (to_v): LoraInjectedLinear(\n",
       "                  (linear): Linear(in_features=768, out_features=640, bias=False)\n",
       "                  (lora_down): Linear(in_features=768, out_features=32, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lora_up): Linear(in_features=32, out_features=640, bias=False)\n",
       "                  (selector): Identity()\n",
       "                )\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoraInjectedLinear(\n",
       "                    (linear): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (lora_down): Linear(in_features=640, out_features=32, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_up): Linear(in_features=32, out_features=640, bias=False)\n",
       "                    (selector): Identity()\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): LoraInjectedLinear(\n",
       "                      (linear): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                      (lora_down): Linear(in_features=640, out_features=32, bias=False)\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      (lora_up): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                      (selector): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): LoraInjectedLinear(\n",
       "                    (linear): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                    (lora_down): Linear(in_features=2560, out_features=32, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (lora_up): Linear(in_features=32, out_features=640, bias=False)\n",
       "                    (selector): Identity()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock3D(\n",
       "          (norm1): InflatedGroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): InflatedConv3d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): InflatedGroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): InflatedConv3d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): InflatedConv3d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock3D(\n",
       "          (norm1): InflatedGroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): InflatedConv3d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): InflatedGroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): InflatedConv3d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (audio_modules): ModuleList(\n",
       "        (0): Transformer3DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): AudioTemporalBasicTransformerBlock(\n",
       "              (zero_conv_full): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_face): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_lip): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2_0): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Transformer3DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): AudioTemporalBasicTransformerBlock(\n",
       "              (zero_conv_full): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_face): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_lip): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2_0): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (motion_modules): ModuleList(\n",
       "        (0-1): 2 x VanillaTemporalModule(\n",
       "          (temporal_transformer): TemporalTransformer3DModel(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): TemporalTransformerBlock(\n",
       "                (attention_blocks): ModuleList(\n",
       "                  (0-1): 2 x VersatileAttention(\n",
       "                    (Module Info) Attention_Mode: Temporal, Is_Cross_Attention: False\n",
       "                    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (pos_encoder): PositionalEncoding(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (norms): ModuleList(\n",
       "                  (0-1): 2 x LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (ff_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample3D(\n",
       "          (conv): InflatedConv3d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnDownBlock3D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Transformer3DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): TemporalBasicTransformerBlock(\n",
       "              (attn1): Attention(\n",
       "                (to_q): LoraInjectedLinear(\n",
       "                  (linear): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (lora_down): Linear(in_features=1280, out_features=32, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lora_up): Linear(in_features=32, out_features=1280, bias=False)\n",
       "                  (selector): Identity()\n",
       "                )\n",
       "                (to_k): LoraInjectedLinear(\n",
       "                  (linear): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (lora_down): Linear(in_features=1280, out_features=32, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lora_up): Linear(in_features=32, out_features=1280, bias=False)\n",
       "                  (selector): Identity()\n",
       "                )\n",
       "                (to_v): LoraInjectedLinear(\n",
       "                  (linear): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (lora_down): Linear(in_features=1280, out_features=32, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lora_up): Linear(in_features=32, out_features=1280, bias=False)\n",
       "                  (selector): Identity()\n",
       "                )\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Transformer3DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): TemporalBasicTransformerBlock(\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock3D(\n",
       "          (norm1): InflatedGroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): InflatedConv3d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): InflatedGroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): InflatedConv3d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): InflatedConv3d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock3D(\n",
       "          (norm1): InflatedGroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): InflatedConv3d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): InflatedGroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): InflatedConv3d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (audio_modules): ModuleList(\n",
       "        (0): Transformer3DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): AudioTemporalBasicTransformerBlock(\n",
       "              (zero_conv_full): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_face): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_lip): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2_0): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Transformer3DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): AudioTemporalBasicTransformerBlock(\n",
       "              (zero_conv_full): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_face): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_lip): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2_0): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (motion_modules): ModuleList(\n",
       "        (0-1): 2 x VanillaTemporalModule(\n",
       "          (temporal_transformer): TemporalTransformer3DModel(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): TemporalTransformerBlock(\n",
       "                (attention_blocks): ModuleList(\n",
       "                  (0-1): 2 x VersatileAttention(\n",
       "                    (Module Info) Attention_Mode: Temporal, Is_Cross_Attention: False\n",
       "                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (pos_encoder): PositionalEncoding(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (norms): ModuleList(\n",
       "                  (0-1): 2 x LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (ff_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample3D(\n",
       "          (conv): InflatedConv3d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DownBlock3D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock3D(\n",
       "          (norm1): InflatedGroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): InflatedConv3d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): InflatedGroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): InflatedConv3d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (motion_modules): ModuleList(\n",
       "        (0-1): 2 x VanillaTemporalModule(\n",
       "          (temporal_transformer): TemporalTransformer3DModel(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): TemporalTransformerBlock(\n",
       "                (attention_blocks): ModuleList(\n",
       "                  (0-1): 2 x VersatileAttention(\n",
       "                    (Module Info) Attention_Mode: Temporal, Is_Cross_Attention: False\n",
       "                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (pos_encoder): PositionalEncoding(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (norms): ModuleList(\n",
       "                  (0-1): 2 x LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (ff_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): UpBlock3D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-2): 3 x ResnetBlock3D(\n",
       "          (norm1): InflatedGroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): InflatedConv3d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): InflatedGroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): InflatedConv3d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): InflatedConv3d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (motion_modules): ModuleList(\n",
       "        (0-2): 3 x VanillaTemporalModule(\n",
       "          (temporal_transformer): TemporalTransformer3DModel(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): TemporalTransformerBlock(\n",
       "                (attention_blocks): ModuleList(\n",
       "                  (0-1): 2 x VersatileAttention(\n",
       "                    (Module Info) Attention_Mode: Temporal, Is_Cross_Attention: False\n",
       "                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (pos_encoder): PositionalEncoding(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (norms): ModuleList(\n",
       "                  (0-1): 2 x LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (ff_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample3D(\n",
       "          (conv): InflatedConv3d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnUpBlock3D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer3DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): TemporalBasicTransformerBlock(\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock3D(\n",
       "          (norm1): InflatedGroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): InflatedConv3d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): InflatedGroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): InflatedConv3d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): InflatedConv3d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock3D(\n",
       "          (norm1): InflatedGroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): InflatedConv3d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): InflatedGroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): InflatedConv3d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): InflatedConv3d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (audio_modules): ModuleList(\n",
       "        (0-2): 3 x Transformer3DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): AudioTemporalBasicTransformerBlock(\n",
       "              (zero_conv_full): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_face): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_lip): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2_0): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (motion_modules): ModuleList(\n",
       "        (0-2): 3 x VanillaTemporalModule(\n",
       "          (temporal_transformer): TemporalTransformer3DModel(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): TemporalTransformerBlock(\n",
       "                (attention_blocks): ModuleList(\n",
       "                  (0-1): 2 x VersatileAttention(\n",
       "                    (Module Info) Attention_Mode: Temporal, Is_Cross_Attention: False\n",
       "                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (pos_encoder): PositionalEncoding(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (norms): ModuleList(\n",
       "                  (0-1): 2 x LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (ff_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample3D(\n",
       "          (conv): InflatedConv3d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnUpBlock3D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer3DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): TemporalBasicTransformerBlock(\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock3D(\n",
       "          (norm1): InflatedGroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): InflatedConv3d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): InflatedGroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): InflatedConv3d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): InflatedConv3d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock3D(\n",
       "          (norm1): InflatedGroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): InflatedConv3d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): InflatedGroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): InflatedConv3d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): InflatedConv3d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock3D(\n",
       "          (norm1): InflatedGroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): InflatedConv3d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): InflatedGroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): InflatedConv3d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): InflatedConv3d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (audio_modules): ModuleList(\n",
       "        (0-2): 3 x Transformer3DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): AudioTemporalBasicTransformerBlock(\n",
       "              (zero_conv_full): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_face): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_lip): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2_0): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (motion_modules): ModuleList(\n",
       "        (0-2): 3 x VanillaTemporalModule(\n",
       "          (temporal_transformer): TemporalTransformer3DModel(\n",
       "            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): TemporalTransformerBlock(\n",
       "                (attention_blocks): ModuleList(\n",
       "                  (0-1): 2 x VersatileAttention(\n",
       "                    (Module Info) Attention_Mode: Temporal, Is_Cross_Attention: False\n",
       "                    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (pos_encoder): PositionalEncoding(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (norms): ModuleList(\n",
       "                  (0-1): 2 x LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (ff_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample3D(\n",
       "          (conv): InflatedConv3d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): CrossAttnUpBlock3D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer3DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): TemporalBasicTransformerBlock(\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock3D(\n",
       "          (norm1): InflatedGroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): InflatedConv3d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): InflatedGroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): InflatedConv3d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): InflatedConv3d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x ResnetBlock3D(\n",
       "          (norm1): InflatedGroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): InflatedConv3d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): InflatedGroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): InflatedConv3d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): InflatedConv3d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (audio_modules): ModuleList(\n",
       "        (0-2): 3 x Transformer3DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): AudioTemporalBasicTransformerBlock(\n",
       "              (zero_conv_full): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_face): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (zero_conv_lip): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2_0): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (attn2_2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (motion_modules): ModuleList(\n",
       "        (0-2): 3 x VanillaTemporalModule(\n",
       "          (temporal_transformer): TemporalTransformer3DModel(\n",
       "            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): TemporalTransformerBlock(\n",
       "                (attention_blocks): ModuleList(\n",
       "                  (0-1): 2 x VersatileAttention(\n",
       "                    (Module Info) Attention_Mode: Temporal, Is_Cross_Attention: False\n",
       "                    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (pos_encoder): PositionalEncoding(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (norms): ModuleList(\n",
       "                  (0-1): 2 x LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (ff_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mid_block): UNetMidBlock3DCrossAttn(\n",
       "    (attentions): ModuleList(\n",
       "      (0): Transformer3DModel(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): TemporalBasicTransformerBlock(\n",
       "            (attn1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock3D(\n",
       "        (norm1): InflatedGroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv1): InflatedConv3d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (norm2): InflatedGroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): InflatedConv3d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (audio_modules): ModuleList(\n",
       "      (0): Transformer3DModel(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): AudioTemporalBasicTransformerBlock(\n",
       "            (zero_conv_full): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (zero_conv_face): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (zero_conv_lip): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (attn1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2_0): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (attn2_1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (attn2_2): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (motion_modules): ModuleList(\n",
       "      (0): VanillaTemporalModule(\n",
       "        (temporal_transformer): TemporalTransformer3DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): TemporalTransformerBlock(\n",
       "              (attention_blocks): ModuleList(\n",
       "                (0-1): 2 x VersatileAttention(\n",
       "                  (Module Info) Attention_Mode: Temporal, Is_Cross_Attention: False\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (pos_encoder): PositionalEncoding(\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (norms): ModuleList(\n",
       "                (0-1): 2 x LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (ff_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_norm_out): InflatedGroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "  (conv_act): SiLU()\n",
       "  (conv_out): InflatedConv3d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoising_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hallo'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_pipeline: hallo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if use_pipeline == 'emo':\n",
    "    print('use_pipeline: emo')\n",
    "    pipeline = FaceAnimatePipeline(\n",
    "            vae=vae,\n",
    "            reference_unet=reference_unet,\n",
    "            denoising_unet=denoising_unet,\n",
    "            face_locator=face_locator,\n",
    "            image_encoder=image_enc,\n",
    "            speed_encoder=speed_enc,\n",
    "            scheduler=val_noise_scheduler,\n",
    "        )\n",
    "if use_pipeline == 'hallo':\n",
    "    print('use_pipeline: hallo')\n",
    "    pipeline = FaceAnimatePipeline(\n",
    "            vae=vae,\n",
    "            reference_unet=reference_unet,\n",
    "            denoising_unet=denoising_unet,\n",
    "            face_locator=face_locator,\n",
    "            image_proj=imageproj,\n",
    "            scheduler=val_noise_scheduler,\n",
    "        )\n",
    "pipeline.to(device=device, dtype=weight_dtype)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel_values_ref_img = torch.randn([1, 3, 3, 512, 512]).to(device=device, dtype=weight_dtype)\n",
    "# audio_tensor = torch.randn([1, 16, 32, 768]).to(device=device, dtype=weight_dtype)\n",
    "# source_image_face_emb = torch.randn([1, 512]).to(device=device, dtype=weight_dtype)\n",
    "# source_image_face_region = torch.randn([1, 3, 512, 512]).to(device=device, dtype=weight_dtype)\n",
    "# source_image_clip_img = torch.randn([1, 3, 224, 224]).to(device=device, dtype=weight_dtype)\n",
    "# head_speed = torch.randn([1, 16]).to(device=device, dtype=weight_dtype)\n",
    "# source_image_face_emb = torch.randn([1, 512]).to(device=device, dtype=weight_dtype)\n",
    "# img_size = 512, 512\n",
    "# clip_length = 16\n",
    "# generator = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'tunable_op_max_tuning_duration_ms': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv_use_max_workspace': '1', 'use_tf32': '1', 'cudnn_conv1d_pad_to_nc1d': '0', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'gpu_external_alloc': '0', 'gpu_mem_limit': '18446744073709551615', 'arena_extend_strategy': 'kNextPowerOfTwo', 'user_compute_stream': '0', 'has_user_compute_stream': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0'}}\n",
      "find model: ./pretrained_models/face_analysis/models/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'tunable_op_max_tuning_duration_ms': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv_use_max_workspace': '1', 'use_tf32': '1', 'cudnn_conv1d_pad_to_nc1d': '0', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'gpu_external_alloc': '0', 'gpu_mem_limit': '18446744073709551615', 'arena_extend_strategy': 'kNextPowerOfTwo', 'user_compute_stream': '0', 'has_user_compute_stream': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0'}}\n",
      "find model: ./pretrained_models/face_analysis/models/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'tunable_op_max_tuning_duration_ms': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv_use_max_workspace': '1', 'use_tf32': '1', 'cudnn_conv1d_pad_to_nc1d': '0', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'gpu_external_alloc': '0', 'gpu_mem_limit': '18446744073709551615', 'arena_extend_strategy': 'kNextPowerOfTwo', 'user_compute_stream': '0', 'has_user_compute_stream': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0'}}\n",
      "find model: ./pretrained_models/face_analysis/models/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'tunable_op_max_tuning_duration_ms': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv_use_max_workspace': '1', 'use_tf32': '1', 'cudnn_conv1d_pad_to_nc1d': '0', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'gpu_external_alloc': '0', 'gpu_mem_limit': '18446744073709551615', 'arena_extend_strategy': 'kNextPowerOfTwo', 'user_compute_stream': '0', 'has_user_compute_stream': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0'}}\n",
      "find model: ./pretrained_models/face_analysis/models/glintr100.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'tunable_op_max_tuning_duration_ms': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv_use_max_workspace': '1', 'use_tf32': '1', 'cudnn_conv1d_pad_to_nc1d': '0', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'gpu_external_alloc': '0', 'gpu_mem_limit': '18446744073709551615', 'arena_extend_strategy': 'kNextPowerOfTwo', 'user_compute_stream': '0', 'has_user_compute_stream': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0'}}\n",
      "find model: ./pretrained_models/face_analysis/models/scrfd_10g_bnkps.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2VecModel were not initialized from the model checkpoint at ./pretrained_models/wav2vec/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-10-18 19:34:12,327 - INFO - separator - Separator version 0.17.2 instantiating with output_dir: ./.cache/audio_preprocess, output_format: WAV\n",
      "2024-10-18 19:34:12,343 - INFO - separator - Operating System: Linux #1 SMP PREEMPT_DYNAMIC Wed Jul 31 15:28:35 UTC 2024\n",
      "2024-10-18 19:34:12,344 - INFO - separator - System: Linux Node: research-workstation-849 Release: 5.14.0-427.28.1.el9_4.x86_64 Machine: x86_64 Proc: x86_64\n",
      "2024-10-18 19:34:12,344 - INFO - separator - Python Version: 3.10.15\n",
      "2024-10-18 19:34:12,344 - INFO - separator - PyTorch Version: 2.2.2+cu121\n",
      "2024-10-18 19:34:12,402 - INFO - separator - FFmpeg installed: ffmpeg version 5.1.6 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "2024-10-18 19:34:12,404 - INFO - separator - ONNX Runtime GPU package installed with version: 1.18.0\n",
      "2024-10-18 19:34:12,405 - INFO - separator - CUDA is available in Torch, setting Torch device to CUDA\n",
      "2024-10-18 19:34:12,405 - INFO - separator - ONNXruntime has CUDAExecutionProvider available, enabling acceleration\n",
      "2024-10-18 19:34:12,405 - INFO - separator - Loading model Kim_Vocal_2.onnx...\n",
      "2024-10-18 19:34:13,197 - INFO - separator - Load model duration: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "width, height = 512, 512\n",
    "\n",
    "image_processor = ImageProcessor((width, height), \"./pretrained_models/face_analysis\")\n",
    "audio_processor = AudioProcessor(\n",
    "    16000,\n",
    "    25,\n",
    "    \"./pretrained_models/wav2vec/wav2vec2-base-960h\",\n",
    "    False,\n",
    "    os.path.dirname(\"./pretrained_models/audio_separator/Kim_Vocal_2.onnx\"),\n",
    "    os.path.basename(\"./pretrained_models/audio_separator/Kim_Vocal_2.onnx\"),\n",
    "    os.path.join('./', '.cache', \"audio_preprocess\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729280056.506137   35656 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1729280056.592072   36623 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 550.54.17), renderer: NVIDIA A10G/PCIe/SSE2\n",
      "W0000 00:00:1729280056.594760   35656 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1729280056.620806   36625 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729280056.630640   36628 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-10-18 19:34:16,754 - INFO - separator - Starting separation process for audio_file_path: examples/driving_audios/1.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: ./.cache/1_sep_background.png\n",
      "Processed and saved: ./.cache/1_sep_face.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.06s/it]\n",
      "100%|██████████| 3/3 [00:00<00:00, 12.00it/s]\n",
      "2024-10-18 19:34:24,121 - INFO - mdx_separator - Saving Vocals stem to 1_(Vocals)_Kim_Vocal_2.wav...\n",
      "2024-10-18 19:34:24,325 - INFO - common_separator - Clearing input audio file paths, sources and stems...\n",
      "2024-10-18 19:34:24,326 - INFO - separator - Separation duration: 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/12]\n",
      "store attn map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:38<00:00,  1.56s/it]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/12]\n",
      "store attn map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:38<00:00,  1.55s/it]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/12]\n",
      "store attn map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:38<00:00,  1.55s/it]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/12]\n",
      "store attn map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:38<00:00,  1.55s/it]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/12]\n",
      "store attn map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:38<00:00,  1.55s/it]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/12]\n",
      "store attn map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:38<00:00,  1.55s/it]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/12]\n",
      "store attn map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:38<00:00,  1.55s/it]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/12]\n",
      "store attn map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:38<00:00,  1.55s/it]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/12]\n",
      "store attn map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:38<00:00,  1.55s/it]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/12]\n",
      "store attn map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:38<00:00,  1.55s/it]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/12]\n",
      "store attn map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:38<00:00,  1.55s/it]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/12]\n",
      "store attn map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:38<00:00,  1.55s/it]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video ./test_1_1.mp4.\n",
      "MoviePy - Writing audio in test_1_1TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video ./test_1_1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./test_1_1.mp4\n"
     ]
    }
   ],
   "source": [
    "# RUN hallo\n",
    "idx = 0\n",
    "ref_img_path = \"examples/reference_images/1.jpg\"\n",
    "audio_path = \"examples/driving_audios/1.wav\"\n",
    "clip_length = 16\n",
    "img_size = 512, 512\n",
    "face_expand_ratio = 1.2\n",
    "save_dir = './'\n",
    "\n",
    "audio_path = cfg.audio_path[idx]\n",
    "source_image_pixels, \\\n",
    "source_image_face_region, \\\n",
    "source_image_face_emb, \\\n",
    "source_image_full_mask, \\\n",
    "source_image_face_mask, \\\n",
    "source_image_lip_mask = image_processor.preprocess(\n",
    "    ref_img_path, os.path.join(save_dir, '.cache'), cfg.face_expand_ratio)\n",
    "audio_emb, audio_length = audio_processor.preprocess(\n",
    "    audio_path, clip_length)\n",
    "\n",
    "audio_emb = process_audio_emb(audio_emb)\n",
    "\n",
    "source_image_pixels = source_image_pixels.unsqueeze(0)\n",
    "source_image_face_region = source_image_face_region.unsqueeze(0)\n",
    "source_image_face_emb = source_image_face_emb.reshape(1, -1)\n",
    "source_image_face_emb = torch.tensor(source_image_face_emb)\n",
    "\n",
    "source_image_full_mask = [\n",
    "    (mask.repeat(clip_length, 1))\n",
    "    for mask in source_image_full_mask\n",
    "]\n",
    "source_image_face_mask = [\n",
    "    (mask.repeat(clip_length, 1))\n",
    "    for mask in source_image_face_mask\n",
    "]\n",
    "source_image_lip_mask = [\n",
    "    (mask.repeat(clip_length, 1))\n",
    "    for mask in source_image_lip_mask\n",
    "]\n",
    "\n",
    "times = audio_emb.shape[0] // clip_length\n",
    "tensor_result = []\n",
    "generator = torch.manual_seed(42)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    for t in range(times):\n",
    "        print(f\"[{t+1}/{times}]\")\n",
    "\n",
    "        if len(tensor_result) == 0:\n",
    "            # The first iteration\n",
    "            motion_zeros = source_image_pixels.repeat(\n",
    "                cfg.data.n_motion_frames, 1, 1, 1)\n",
    "            motion_zeros = motion_zeros.to(\n",
    "                dtype=source_image_pixels.dtype, device=source_image_pixels.device)\n",
    "            pixel_values_ref_img = torch.cat(\n",
    "                [source_image_pixels, motion_zeros], dim=0)  # concat the ref image and the first motion frames\n",
    "        else:\n",
    "            motion_frames = tensor_result[-1][0]\n",
    "            motion_frames = motion_frames.permute(1, 0, 2, 3)\n",
    "            motion_frames = motion_frames[0 - cfg.data.n_motion_frames:]\n",
    "            motion_frames = motion_frames * 2.0 - 1.0\n",
    "            motion_frames = motion_frames.to(\n",
    "                dtype=source_image_pixels.dtype, device=source_image_pixels.device)\n",
    "            pixel_values_ref_img = torch.cat(\n",
    "                [source_image_pixels, motion_frames], dim=0)  # concat the ref image and the motion frames\n",
    "\n",
    "        pixel_values_ref_img = pixel_values_ref_img.unsqueeze(0)\n",
    "\n",
    "        audio_tensor = audio_emb[\n",
    "            t * clip_length: min((t + 1) * clip_length, audio_emb.shape[0])\n",
    "        ]\n",
    "        audio_tensor = audio_tensor.unsqueeze(0)\n",
    "        audio_tensor = audio_tensor.to(\n",
    "            device=audioproj.device, \n",
    "            dtype=audioproj.dtype\n",
    "        )\n",
    "        audio_tensor = audioproj(audio_tensor)\n",
    "\n",
    "        pipeline_output = pipeline(\n",
    "            ref_image=pixel_values_ref_img,\n",
    "            audio_tensor=audio_tensor,\n",
    "            face_emb=source_image_face_emb,\n",
    "            face_mask=source_image_face_region,\n",
    "            pixel_values_full_mask=source_image_full_mask,\n",
    "            pixel_values_face_mask=source_image_face_mask,\n",
    "            pixel_values_lip_mask=source_image_lip_mask,\n",
    "            width=cfg.data.train_width,\n",
    "            height=cfg.data.train_height,\n",
    "            video_length=clip_length,\n",
    "            num_inference_steps=25,\n",
    "            # num_inference_steps=cfg.inference_steps,\n",
    "            guidance_scale=cfg.cfg_scale,\n",
    "            generator=generator,\n",
    "            store_attn_map=True,\n",
    "        )\n",
    "        tensor_result.append(pipeline_output.videos)\n",
    "\n",
    "tensor_result = torch.cat(tensor_result, dim=2)\n",
    "tensor_result = tensor_result.squeeze(0)\n",
    "tensor_result = tensor_result[:, :audio_length]\n",
    "audio_name = os.path.basename(audio_path).split('.')[0]\n",
    "ref_name = os.path.basename(ref_img_path).split('.')[0]\n",
    "output_file = os.path.join(save_dir,f\"test_{ref_name}_{audio_name}.mp4\")\n",
    "# save the result after all iteration\n",
    "tensor_to_video(tensor_result, output_file, audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention maps\n",
    "torch.cuda.empty_cache()\n",
    "pipeline.attn_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_img_path = \"examples/reference_images/1.jpg\"\n",
    "audio_path = \"examples/driving_audios/1.wav\"\n",
    "clip_length = 16\n",
    "img_size = 512, 512\n",
    "face_expand_ratio = 1.2\n",
    "\n",
    "source_image_pixels, \\\n",
    "source_image_face_region, \\\n",
    "source_image_face_emb, \\\n",
    "source_image_clip_img = image_processor.preprocess(\n",
    "    ref_img_path, \n",
    "    os.path.join('./', '.cache'), \n",
    "    face_expand_ratio\n",
    ")\n",
    "audio_emb, audio_length = audio_processor.preprocess(audio_path, clip_length)\n",
    "\n",
    "audio_emb = process_audio_emb(audio_emb)\n",
    "\n",
    "source_image_pixels = source_image_pixels.unsqueeze(0)\n",
    "source_image_face_region = source_image_face_region.unsqueeze(0)\n",
    "source_image_face_emb = source_image_face_emb.reshape(1, -1)\n",
    "source_image_face_emb = torch.tensor(source_image_face_emb)\n",
    "# source_image_clip_img = source_image_clip_img.repeat(3, 1, 1, 1)\n",
    "\n",
    "times = audio_emb.shape[0] // clip_length\n",
    "print(times)\n",
    "\n",
    "# motion_zeros = source_image_pixels.repeat(2, 1, 1, 1)\n",
    "# motion_zeros = motion_zeros.to(\n",
    "#         dtype=source_image_pixels.dtype, \n",
    "#         device=source_image_pixels.device\n",
    "#     )\n",
    "# pixel_values_ref_img = torch.cat(\n",
    "#     [source_image_pixels, motion_zeros], \n",
    "#     dim=0\n",
    "# )  # concat the ref image and the first motion frames\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tensor_result = []\n",
    "for t in range(times):\n",
    "    print(f\"[{t+1}/{times}]\")\n",
    "\n",
    "    if len(tensor_result) == 0:\n",
    "        # The first iteration\n",
    "        motion_zeros = source_image_pixels.repeat(2, 1, 1, 1)\n",
    "        motion_zeros = motion_zeros.to(\n",
    "            dtype=source_image_pixels.dtype, device=source_image_pixels.device)\n",
    "        pixel_values_ref_img = torch.cat(\n",
    "            [source_image_pixels, motion_zeros], dim=0)  # concat the ref image and the first motion frames\n",
    "    else:\n",
    "        motion_frames = tensor_result[-1][0]\n",
    "        motion_frames = motion_frames.permute(1, 0, 2, 3)\n",
    "        motion_frames = motion_frames[0 - 2:]\n",
    "        motion_frames = motion_frames * 2.0 - 1.0\n",
    "        motion_frames = motion_frames.to(\n",
    "            dtype=source_image_pixels.dtype, device=source_image_pixels.device)\n",
    "        pixel_values_ref_img = torch.cat(\n",
    "            [source_image_pixels, motion_frames], dim=0)  # concat the ref image and the motion frames\n",
    "\n",
    "    pixel_values_ref_img = pixel_values_ref_img.unsqueeze(0)\n",
    "\n",
    "    audio_tensor = audio_emb[\n",
    "        t * clip_length: min((t + 1) * clip_length, audio_emb.shape[0])\n",
    "    ]\n",
    "    audio_tensor = audio_tensor.unsqueeze(0)\n",
    "    audio_tensor = audio_tensor.to(\n",
    "        device=audioproj.device, dtype=audioproj.dtype\n",
    "    )\n",
    "    audio_tensor = audioproj(audio_tensor)\n",
    "\n",
    "    speed_emb = torch.ones(audio_tensor.shape[:2]).to(\n",
    "        dtype=speed_enc.dtype, device=speed_enc.device\n",
    "    )\n",
    "    # print(speed_emb.shape)\n",
    "    # break\n",
    "    # print(pixel_values_ref_img.shape)\n",
    "    # print(source_image_face_emb.shape)\n",
    "    # print(audio_tensor.shape)\n",
    "    # print(source_image_face_region.shape)\n",
    "    # print(source_image_clip_img.shape)\n",
    "    # print(width)\n",
    "    # print(height)\n",
    "    # print(speed_emb.shape)\n",
    "    # print(clip_length)\n",
    "\n",
    "    pipeline_output = pipeline(\n",
    "        ref_image=pixel_values_ref_img,\n",
    "        face_emb=source_image_face_emb,\n",
    "        audio_tensor=audio_tensor,\n",
    "        face_mask=source_image_face_region,\n",
    "        clip_img=source_image_clip_img,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        speed_emb=speed_emb,\n",
    "        video_length=clip_length,\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=3.5,\n",
    "        generator=torch.manual_seed(42),\n",
    "    )\n",
    "\n",
    "    tensor_result.append(pipeline_output.videos)\n",
    "    # break\n",
    "\n",
    "tensor_result = torch.cat(tensor_result, dim=2)\n",
    "tensor_result = tensor_result.squeeze(0)\n",
    "tensor_result = tensor_result[:, :audio_length]\n",
    "audio_name = os.path.basename(audio_path).split('.')[0]\n",
    "ref_name = os.path.basename(ref_img_path).split('.')[0]\n",
    "\n",
    "output_file = os.path.join('./',f\"test_{ref_name}_{audio_name}.mp4\")\n",
    "# save the result after all iteration\n",
    "tensor_to_video(tensor_result, output_file, audio_path)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# clip_img torch.Size([1, 3, 224, 224])\n",
    "# latents torch.Size([1, 4, 16, 64, 64])\n",
    "# ref_image_latents torch.Size([3, 4, 64, 64])\n",
    "# face_mask torch.Size([2, 320, 16, 64, 64])\n",
    "# audio_tensor torch.Size([2, 16, 32, 768])\n",
    "# speed_emb torch.Size([2, 16, 768])\n",
    "# encoder_hidden_states torch.Size([2, 1, 768])\n",
    "# encoder_hidden_states.repeat torch.Size([6, 1, 768])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.reference_control_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(f\"test_{ref_name}_{audio_name}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values_ref_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_tensor.shape\n",
    "# speed_emb.shape\n",
    "# speed_emb = torch.ones(audio_tensor.shape[:2]).to(\n",
    "#     dtype=speed_enc.dtype, device=speed_enc.device\n",
    "# )\n",
    "print(speed_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = pipeline\n",
    "ref_image=pixel_values_ref_img # [1, 3, 3, 512, 512]\n",
    "audio_tensor=audio_tensor # [1, 16, 32, 768]\n",
    "face_emb=source_image_face_emb # [1, 512]\n",
    "face_mask=source_image_face_region # [1, 3, 512, 512]\n",
    "clip_img=source_image_clip_img # [1, 3, 224, 224]\n",
    "speed_emb=speed_emb\n",
    "width=width\n",
    "height=height\n",
    "video_length=clip_length\n",
    "num_inference_steps=25\n",
    "guidance_scale=3.5\n",
    "generator=torch.manual_seed(42)\n",
    "num_images_per_prompt=1\n",
    "output_type: Optional[str] = \"tensor\"\n",
    "eta: float = 0.0\n",
    "\n",
    "\n",
    "# Default height and width to unet\n",
    "height = height or self.unet.config.sample_size * self.vae_scale_factor\n",
    "width = width or self.unet.config.sample_size * self.vae_scale_factor\n",
    "\n",
    "device = self._execution_device\n",
    "\n",
    "do_classifier_free_guidance = guidance_scale > 1.0\n",
    "# do_classifier_free_guidance = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Prepare timesteps\n",
    "    self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    timesteps = self.scheduler.timesteps\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# prepare clip image embeddings\n",
    "with torch.no_grad():\n",
    "    clip_image_embeds = clip_img.to(self.image_encoder.device, self.image_encoder.dtype)\n",
    "    encoder_hidden_states = self.image_encoder(\n",
    "        clip_image_embeds\n",
    "    ).image_embeds.unsqueeze(1)\n",
    "    uncond_encoder_hidden_states = self.image_encoder(\n",
    "        torch.zeros_like(clip_image_embeds)\n",
    "    ).image_embeds.unsqueeze()\n",
    "\n",
    "if do_classifier_free_guidance:\n",
    "    encoder_hidden_states = torch.cat(\n",
    "        [uncond_encoder_hidden_states, encoder_hidden_states], dim=0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_control_writer = ReferenceAttentionControl(\n",
    "    self.reference_unet,\n",
    "    do_classifier_free_guidance=do_classifier_free_guidance,\n",
    "    mode=\"write\",\n",
    "    batch_size=batch_size,\n",
    "    fusion_blocks=\"full\",\n",
    "    # return_modules=True,\n",
    ")\n",
    "reference_control_reader = ReferenceAttentionControl(\n",
    "    self.denoising_unet,\n",
    "    do_classifier_free_guidance=do_classifier_free_guidance,\n",
    "    mode=\"read\",\n",
    "    batch_size=batch_size,\n",
    "    fusion_blocks=\"full\",\n",
    "    # return_modules=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_channels_latents = self.denoising_unet.in_channels\n",
    "\n",
    "with torch.no_grad():\n",
    "    latents = self.prepare_latents(\n",
    "        batch_size * num_images_per_prompt,\n",
    "        num_channels_latents,\n",
    "        width,\n",
    "        height,\n",
    "        video_length,\n",
    "        clip_image_embeds.dtype,\n",
    "        device,\n",
    "        generator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare extra step kwargs.\n",
    "extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "# Prepare ref image latents\n",
    "with torch.no_grad():\n",
    "    ref_image_tensor = rearrange(ref_image, \"b f c h w -> (b f) c h w\")\n",
    "    ref_image_tensor = self.ref_image_processor.preprocess(ref_image_tensor, height=height, width=width)  # (bs, c, width, height)\n",
    "    ref_image_tensor = ref_image_tensor.to(dtype=self.vae.dtype, device=self.vae.device)\n",
    "    ref_image_latents = self.vae.encode(ref_image_tensor).latent_dist.mean\n",
    "    ref_image_latents = ref_image_latents * 0.18215  # (b, 4, h, w)\n",
    "\n",
    "with torch.no_grad():\n",
    "    face_mask = face_mask.unsqueeze(1).to(dtype=self.face_locator.dtype, device=self.face_locator.device) # (bs, f, c, H, W)\n",
    "    face_mask = repeat(face_mask, \"b f c h w -> b (repeat f) c h w\", repeat=video_length)\n",
    "    face_mask = face_mask.transpose(1, 2)  # (bs, c, f, H, W)\n",
    "    face_mask = self.face_locator(face_mask)\n",
    "    face_mask = torch.cat([torch.zeros_like(face_mask), face_mask], dim=0) if do_classifier_free_guidance else face_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if do_classifier_free_guidance:\n",
    "    uncond_audio_tensor = torch.zeros_like(audio_tensor)\n",
    "    audio_tensor = torch.cat([uncond_audio_tensor, audio_tensor], dim=0)\n",
    "    audio_tensor = audio_tensor.to(dtype=self.denoising_unet.dtype, device=self.denoising_unet.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if self.speed_encoder is not None:\n",
    "#     if speed_emb is None:\n",
    "#         speed_emb = torch.ones(audio_tensor.shape[0]).to(\n",
    "#             dtype=self.speed_encoder.dtype, device=self.speed_encoder.device\n",
    "#         ).repeat(2,1)\n",
    "#     else:\n",
    "#         uncond_speed_emb = torch.zeros_like(speed_emb)\n",
    "#         speed_emb = torch.cat([uncond_speed_emb, speed_emb], dim=0)\n",
    "        \n",
    "#     with torch.no_grad():\n",
    "#         speed_emb = self.speed_encoder(speed_emb)\n",
    "\n",
    "if speed_emb is None:\n",
    "    speed_emb = torch.ones(audio_tensor.shape[0]).to(\n",
    "        dtype=self.speed_encoder.dtype, device=self.speed_encoder.device\n",
    "    )\n",
    "\n",
    "if do_classifier_free_guidance:\n",
    "    # uncond_speed_emb = torch.zeros_like(speed_emb)\n",
    "    # speed_emb = torch.cat([uncond_speed_emb, speed_emb], dim=0)\n",
    "    speed_emb = speed_emb.repeat(2, 1)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    speed_emb = self.speed_encoder(speed_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"clip_img\", clip_img.shape)\n",
    "print(\"latents\", latents.shape)\n",
    "print(\"ref_image_latents\", ref_image_latents.shape)\n",
    "print(\"face_mask\", face_mask.shape)\n",
    "print(\"audio_tensor\", audio_tensor.shape)\n",
    "print(\"speed_emb\", speed_emb.shape)\n",
    "\n",
    "print(\"encoder_hidden_states\", encoder_hidden_states.shape)\n",
    "print(\"encoder_hidden_states.repeat\", encoder_hidden_states.repeat(ref_image_latents.shape[0], 1, 1).shape)\n",
    "\n",
    "# clip_img torch.Size([1, 3, 224, 224])\n",
    "# latents torch.Size([1, 4, 16, 64, 64])\n",
    "# ref_image_latents torch.Size([3, 4, 64, 64])\n",
    "# face_mask torch.Size([2, 320, 16, 64, 64])\n",
    "# audio_tensor torch.Size([2, 16, 32, 768])\n",
    "# speed_emb torch.Size([2, 16, 768])\n",
    "# encoder_hidden_states torch.Size([2, 1, 768])\n",
    "\n",
    "# encoder_hidden_states.repeat torch.Size([6, 1, 768])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timesteps\n",
    "\n",
    "# denoising loop\n",
    "num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "i = 0\n",
    "t = timesteps[i]\n",
    "    # for i, t in enumerate(timesteps):\n",
    "# Forward reference image\n",
    "if i == 0:\n",
    "    with torch.no_grad():\n",
    "        self.reference_unet(\n",
    "            ref_image_latents.repeat(\n",
    "                (2 if do_classifier_free_guidance else 1), 1, 1, 1\n",
    "            ),\n",
    "            torch.zeros_like(t),\n",
    "            encoder_hidden_states=encoder_hidden_states.repeat(\n",
    "                ref_image_latents.shape[0], 1, 1\n",
    "            ),\n",
    "            return_dict=False,\n",
    "        )\n",
    "    reference_control_reader.update(reference_control_writer)\n",
    "\n",
    "# expand the latents if we are doing classifier free guidance\n",
    "latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "with torch.no_grad():\n",
    "    noise_pred = self.denoising_unet(\n",
    "        latent_model_input,\n",
    "        t,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        mask_cond_fea=face_mask,\n",
    "        audio_embedding=audio_tensor,\n",
    "        speed_embedding=speed_emb,\n",
    "        return_dict=False,\n",
    "    )[0]\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_attn_modules, writer_attn_modules = reference_control_reader.update(reference_control_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_attn_modules[0].bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_attn_modules[0].attn_score[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models.attention_processor import (\n",
    "    AttnProcessor,\n",
    "    AttnProcessor2_0,\n",
    "    SpatialNorm,\n",
    ")\n",
    "from diffusers.models.attention import (\n",
    "    AdaLayerNorm, \n",
    "    AdaLayerNormZero,\n",
    "    Attention, \n",
    "    FeedForward\n",
    ")\n",
    "\n",
    "class CustomAttention(Attention):\n",
    "    r\"\"\"\n",
    "    A cross attention layer.\n",
    "\n",
    "    Parameters:\n",
    "        query_dim (`int`):\n",
    "            The number of channels in the query.\n",
    "        cross_attention_dim (`int`, *optional*):\n",
    "            The number of channels in the encoder_hidden_states. If not given, defaults to `query_dim`.\n",
    "        heads (`int`,  *optional*, defaults to 8):\n",
    "            The number of heads to use for multi-head attention.\n",
    "        dim_head (`int`,  *optional*, defaults to 64):\n",
    "            The number of channels in each head.\n",
    "        dropout (`float`, *optional*, defaults to 0.0):\n",
    "            The dropout probability to use.\n",
    "        bias (`bool`, *optional*, defaults to False):\n",
    "            Set to `True` for the query, key, and value linear layers to contain a bias parameter.\n",
    "        upcast_attention (`bool`, *optional*, defaults to False):\n",
    "            Set to `True` to upcast the attention computation to `float32`.\n",
    "        upcast_softmax (`bool`, *optional*, defaults to False):\n",
    "            Set to `True` to upcast the softmax computation to `float32`.\n",
    "        cross_attention_norm (`str`, *optional*, defaults to `None`):\n",
    "            The type of normalization to use for the cross attention. Can be `None`, `layer_norm`, or `group_norm`.\n",
    "        cross_attention_norm_num_groups (`int`, *optional*, defaults to 32):\n",
    "            The number of groups to use for the group norm in the cross attention.\n",
    "        added_kv_proj_dim (`int`, *optional*, defaults to `None`):\n",
    "            The number of channels to use for the added key and value projections. If `None`, no projection is used.\n",
    "        norm_num_groups (`int`, *optional*, defaults to `None`):\n",
    "            The number of groups to use for the group norm in the attention.\n",
    "        spatial_norm_dim (`int`, *optional*, defaults to `None`):\n",
    "            The number of channels to use for the spatial normalization.\n",
    "        out_bias (`bool`, *optional*, defaults to `True`):\n",
    "            Set to `True` to use a bias in the output linear layer.\n",
    "        scale_qk (`bool`, *optional*, defaults to `True`):\n",
    "            Set to `True` to scale the query and key by `1 / sqrt(dim_head)`.\n",
    "        only_cross_attention (`bool`, *optional*, defaults to `False`):\n",
    "            Set to `True` to only use cross attention and not added_kv_proj_dim. Can only be set to `True` if\n",
    "            `added_kv_proj_dim` is not `None`.\n",
    "        eps (`float`, *optional*, defaults to 1e-5):\n",
    "            An additional value added to the denominator in group normalization that is used for numerical stability.\n",
    "        rescale_output_factor (`float`, *optional*, defaults to 1.0):\n",
    "            A factor to rescale the output by dividing it with this value.\n",
    "        residual_connection (`bool`, *optional*, defaults to `False`):\n",
    "            Set to `True` to add the residual connection to the output.\n",
    "        _from_deprecated_attn_block (`bool`, *optional*, defaults to `False`):\n",
    "            Set to `True` if the attention block is loaded from a deprecated state dict.\n",
    "        processor (`AttnProcessor`, *optional*, defaults to `None`):\n",
    "            The attention processor to use. If `None`, defaults to `AttnProcessor2_0` if `torch 2.x` is used and\n",
    "            `AttnProcessor` otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_dim: int,\n",
    "        cross_attention_dim: Optional[int] = None,\n",
    "        heads: int = 8,\n",
    "        dim_head: int = 64,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = False,\n",
    "        upcast_attention: bool = False,\n",
    "        upcast_softmax: bool = False,\n",
    "        cross_attention_norm: Optional[str] = None,\n",
    "        cross_attention_norm_num_groups: int = 32,\n",
    "        added_kv_proj_dim: Optional[int] = None,\n",
    "        norm_num_groups: Optional[int] = None,\n",
    "        spatial_norm_dim: Optional[int] = None,\n",
    "        out_bias: bool = True,\n",
    "        scale_qk: bool = True,\n",
    "        only_cross_attention: bool = False,\n",
    "        eps: float = 1e-5,\n",
    "        rescale_output_factor: float = 1.0,\n",
    "        residual_connection: bool = False,\n",
    "        _from_deprecated_attn_block: bool = False,\n",
    "        processor: Optional[\"AttnProcessor\"] = None,\n",
    "        out_dim: int = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            query_dim,\n",
    "            cross_attention_dim,\n",
    "            heads,\n",
    "            dim_head,\n",
    "            dropout,\n",
    "            bias,\n",
    "            upcast_attention,\n",
    "            upcast_softmax,\n",
    "            cross_attention_norm,\n",
    "            cross_attention_norm_num_groups,\n",
    "            added_kv_proj_dim,\n",
    "            norm_num_groups,\n",
    "            spatial_norm_dim,\n",
    "            out_bias,\n",
    "            scale_qk,\n",
    "            only_cross_attention,\n",
    "            eps,\n",
    "            rescale_output_factor,\n",
    "            residual_connection,\n",
    "            _from_deprecated_attn_block,\n",
    "            processor,\n",
    "            out_dim,\n",
    "        )\n",
    "        \n",
    "\n",
    "class CustomAttnProcessor:\n",
    "    r\"\"\"\n",
    "    Default processor for performing attention-related computations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn: Attention,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        temb: Optional[torch.FloatTensor] = None,\n",
    "        return_attn_score: Optional[bool] = False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "        )\n",
    "        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        query = attn.head_to_batch_dim(query)\n",
    "        key = attn.head_to_batch_dim(key)\n",
    "        value = attn.head_to_batch_dim(value)\n",
    "\n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        if return_attn_score:\n",
    "            return hidden_states, attention_probs\n",
    "        else:\n",
    "            return hidden_states\n",
    "    \n",
    "aa = CustomAttention(\n",
    "            query_dim=32,\n",
    "            heads=8,\n",
    "            dim_head=32,\n",
    "            dropout=0.0,\n",
    "            bias=False,\n",
    "            cross_attention_dim=None,\n",
    "            upcast_attention=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    self.reference_unet(\n",
    "        ref_image_latents.repeat(\n",
    "            (2 if do_classifier_free_guidance else 1), 1, 1, 1\n",
    "        ),\n",
    "        torch.zeros_like(t),\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        return_dict=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "    for i, t in enumerate(timesteps):\n",
    "        # Forward reference image\n",
    "        if i == 0:\n",
    "            with torch.no_grad():\n",
    "                self.reference_unet(\n",
    "                    ref_image_latents.repeat(\n",
    "                        (2 if do_classifier_free_guidance else 1), 1, 1, 1\n",
    "                    ),\n",
    "                    torch.zeros_like(t),\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    return_dict=False,\n",
    "                )\n",
    "            reference_control_reader.update(reference_control_writer)\n",
    "        \n",
    "        # encoder_hidden_states :: [2, 4, 768]\n",
    "        # ref_image_latents.shape :: [3, 4, 64, 64]\n",
    "        # ref_image_latents.repeat :: [6, 4, 64, 64]\n",
    "        # latents :: [1, 4, 14, 64, 64]\n",
    "        # audio_tensor :: [2, 14, 32, 768]\n",
    "        # import pdb;pdb.set_trace()\n",
    "\n",
    "        # expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "        latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            noise_pred = self.denoising_unet(\n",
    "                latent_model_input,\n",
    "                t,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                mask_cond_fea=face_mask,\n",
    "                audio_embedding=audio_tensor,\n",
    "                speed_embedding=speed_emb,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "        # perform guidance\n",
    "        if do_classifier_free_guidance:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
    "\n",
    "        # call the callback, if provided\n",
    "        if i == len(timesteps) - 1 or (i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0:\n",
    "            progress_bar.update()\n",
    "\n",
    "    reference_control_reader.clear()\n",
    "    reference_control_writer.clear()\n",
    "\n",
    "# Post-processing\n",
    "images = self.decode_latents(latents)  # (b, c, f, h, w)\n",
    "\n",
    "# Convert to tensor\n",
    "if output_type == \"tensor\":\n",
    "    images = torch.from_numpy(images)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_classifier_free_guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_attn_modules, writer_attn_modules = reference_control_reader.update(reference_control_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(reader_attn_modules)\n",
    "writer_attn_modules[0].bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.min()*255, images.max()*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images.shape\n",
    "torchvision.io.write_video('tmp.mp4',images[0].permute(1,2,3,0)*255,fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Video('tmp.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_output = pipeline(\n",
    "    ref_image=pixel_values_ref_img, # [1, 3, 3, 512, 512]\n",
    "    audio_tensor=audio_tensor, # [1, 16, 32, 768]\n",
    "    face_emb=source_image_face_emb, # [1, 512]\n",
    "    face_mask=source_image_face_region, # [1, 3, 512, 512]\n",
    "    clip_img=source_image_clip_img, # [1, 3, 224, 224]\n",
    "    speed_emb=speed_emb, # [1, 16]\n",
    "    width=width,\n",
    "    height=height,\n",
    "    video_length=clip_length,\n",
    "    num_inference_steps=25,\n",
    "    guidance_scale=3.5,\n",
    "    generator=generator,\n",
    ")\n",
    "\n",
    "# self.reference_unet(ref_image_latents.repeat((2 if do_classifier_free_guidance else 1), 1, 1, 1),torch.zeros_like(t),encoder_hidden_states=encoder_hidden_states,return_dict=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warping self-attention feature\n",
    "# try Curved Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_speed_buckets=8\n",
    "\n",
    "aa = torch.linspace(-math.pi, math.pi, num_speed_buckets).repeat(8)\n",
    "print(aa.shape)\n",
    "print(aa)\n",
    "\n",
    "bb = torch.linspace(0.01, math.pi, num_speed_buckets).repeat_interleave(8)\n",
    "print(bb.shape)\n",
    "print(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_rotation_speed = torch.randn(num_speed_buckets,1)\n",
    "cc = torch.tanh((head_rotation_speed - aa)/bb)\n",
    "cc.shape\n",
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from emo.models.speed_encoder import SpeedEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_enc = SpeedEncoder(4,768)\n",
    "out = speed_enc(torch.randn(4,))\n",
    "print(out.shape)\n",
    "# print(out)\n",
    "\n",
    "out = speed_enc(torch.randn(4,1))\n",
    "print(out.shape)\n",
    "\n",
    "out = speed_enc(torch.randn(4,2,))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scripts.train_stage1_emo import Net\n",
    "from hallo.models.face_locator import FaceLocator\n",
    "from hallo.models.mutual_self_attention import ReferenceAttentionControl\n",
    "from hallo.models.unet_2d_condition import UNet2DConditionModel\n",
    "from hallo.models.unet_3d import UNet3DConditionModel\n",
    "from diffusers import AutoencoderKL, DDIMScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoising_unet = UNet3DConditionModel.from_pretrained_2d(\n",
    "        \"./pretrained_models/stable-diffusion-v1-5/\",\n",
    "        \"\",\n",
    "        subfolder=\"unet\",\n",
    "        unet_additional_kwargs={\n",
    "            \"use_motion_module\": False,\n",
    "            \"unet_use_temporal_attention\": False,\n",
    "        },\n",
    "        use_landmark=False\n",
    "    ).to(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG():\n",
    "    def __init__(self):\n",
    "        self.base_model_path= \"./pretrained_models/stable-diffusion-v1-5/\"\n",
    "        self.vae_model_path= \"./pretrained_models/sd-vae-ft-mse\"\n",
    "        self.face_analysis_model_path= \"./pretrained_models/face_analysis\"\n",
    "        self.face_locator_pretrained=False\n",
    "        \n",
    "cfg = CFG()\n",
    "cfg.face_locator_pretrained\n",
    "weight_dtype=torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "vae = AutoencoderKL.from_pretrained(cfg.vae_model_path).to(\n",
    "    \"cuda\", dtype=weight_dtype\n",
    ")\n",
    "reference_unet = UNet2DConditionModel.from_pretrained(\n",
    "    cfg.base_model_path,\n",
    "    subfolder=\"unet\",\n",
    ").to(device=\"cuda\", dtype=weight_dtype)\n",
    "denoising_unet = UNet3DConditionModel.from_pretrained_2d(\n",
    "    cfg.base_model_path,\n",
    "    \"\",\n",
    "    subfolder=\"unet\",\n",
    "    unet_additional_kwargs={\n",
    "        \"use_motion_module\": False,\n",
    "        \"unet_use_temporal_attention\": False,\n",
    "    },\n",
    "    use_landmark=False\n",
    ").to(device=\"cuda\", dtype=weight_dtype)\n",
    "\n",
    "face_locator = FaceLocator_EMO(out_channels=320).to(device=\"cuda\", dtype=weight_dtype)\n",
    "\n",
    "# Freeze\n",
    "vae.requires_grad_(False)\n",
    "denoising_unet.requires_grad_(False)\n",
    "reference_unet.requires_grad_(False)\n",
    "face_locator.requires_grad_(False)\n",
    "\n",
    "reference_control_writer = ReferenceAttentionControl(\n",
    "    reference_unet,\n",
    "    do_classifier_free_guidance=False,\n",
    "    mode=\"write\",\n",
    "    fusion_blocks=\"full\",\n",
    ")\n",
    "reference_control_reader = ReferenceAttentionControl(\n",
    "    denoising_unet,\n",
    "    do_classifier_free_guidance=False,\n",
    "    mode=\"read\",\n",
    "    fusion_blocks=\"full\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# face_locator_emo = FaceLocator_EMO(out_channels=320).to(device=\"cuda\", dtype=weight_dtype)\n",
    "face_locator = FaceLocator(\n",
    "    conditioning_embedding_channels=320,\n",
    "    conditioning_channels=1,\n",
    "    act='relu',\n",
    ").to(device=\"cuda\", dtype=weight_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(\n",
    "    reference_unet,\n",
    "    denoising_unet,\n",
    "    face_locator,\n",
    "    reference_control_writer,\n",
    "    reference_control_reader\n",
    ").to(dtype=weight_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_latents = torch.randn([1, 4, 1, 64, 64]) * 0.18215\n",
    "timesteps = torch.randint(0, 1000, (1,)).long()\n",
    "ref_image_latents = torch.randn([1 ,4, 64, 64]) * 0.18215\n",
    "\n",
    "face_mask = torch.zeros(1, 1, 1, 512, 512)\n",
    "face_mask[:, :, 100:300, 100:300] = 1\n",
    "\n",
    "print(noisy_latents.shape)\n",
    "print(timesteps.shape)\n",
    "print(ref_image_latents.shape)\n",
    "print(face_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_mask_feature = face_locator(face_mask.cuda())\n",
    "print(face_mask_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(\n",
    "        noisy_latents.cuda(),\n",
    "        timesteps.cuda(),\n",
    "        ref_image_latents.cuda(),\n",
    "        face_mask.cuda(),\n",
    "        uncond_fwd=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, Audio\n",
    "Video('/home/sihun.cha/Work/hallo/.cache/output.mp4')\n",
    "# Audio('/home/sihun.cha/Work/hallo/.cache/audio_preprocess/1_(Vocals)_Kim_Vocal_2.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video('/home/sihun.cha/Work/toydata/vid_256/AdamKinzinger0_0.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image =  torch.zeros(2,1,512,512)\n",
    "image[:,:,100:300,100:300]=1\n",
    "plt.imshow(image.permute(0,2,3,1)[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from hallo.models.face_locator import FaceLocator_EMO\n",
    "\n",
    "class FaceLocator(nn.Module):\n",
    "    def __init__(self, in_channel=1, out_channel=4):\n",
    "        super(FaceLocator, self).__init__()\n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channel, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        # Define the final convolutional layer that outputs a single channel (mask)\n",
    "        self.final_conv = nn.Conv2d(64, out_channel, kernel_size=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Forward pass through the convolutional layers\n",
    "        # Assert that images are of the correct type (floating-point)\n",
    "        assert images.dtype == torch.float32, 'Images must be of type torch.float32'\n",
    "        # Assert that images have 4 dimensions [B, C, H, W]\n",
    "        assert images.ndim == 4, 'Images must have 4 dimensions [B, C, H, W]'\n",
    "\n",
    "        x = F.relu(self.conv1(images))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)  # Shape after pooling: (B, 64, H/8, W/8)\n",
    "        \n",
    "\n",
    "        assert x.size(1) == 64, f\"Input to final conv layer has {x.size(1)} channels, expected 64.\"\n",
    "\n",
    "        # Pass through the final convolutional layer to get a single channel output\n",
    "        logits = self.final_conv(x)  # Output logits directly, Shape: (B, 1, H/8, W/8)\n",
    "        \n",
    "        # No sigmoid or thresholding here because BCEWithLogitsLoss will handle it\n",
    "\n",
    "        # Upsample logits to the size of the original image\n",
    "        # logits = F.interpolate(logits, size=(images.shape[2], images.shape[3]), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# def normalize(v):\n",
    "#     norm = np.linalg.norm(v)\n",
    "#     if norm == 0: \n",
    "#        return v\n",
    "#     return v / norm\n",
    "def normalize(V):\n",
    "    V = (V - (V.max(0).max(0) + V.min(0).min(0)) *0.5) / max(V.max(0).max(0) - V.min(0).min(0))\n",
    "    V = V + 0.5\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = FaceLocator()\n",
    "output = foo(image)\n",
    "print(output.shape)\n",
    "\n",
    "plt.imshow(\n",
    "    normalize(output[0].detach().numpy().transpose(1,2,0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = FaceLocator_EMO(1,4)\n",
    "output = foo(image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(\n",
    "    normalize(output[0].detach().numpy().transpose(1,2,0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hallo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
